{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Experiment Mesh Analysis\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c1ecc4b83731736b"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# external imports\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "from shapely import (\n",
    "    from_geojson\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-13T22:32:04.371554Z",
     "start_time": "2024-03-13T22:32:04.086276Z"
    }
   },
   "id": "c8df69fe46cac602"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# internal imports\n",
    "from src.utils.common_functions import (\n",
    "    json_file_to_dict,\n",
    "    get_list_files_in_path,\n",
    ")\n",
    "from src.utils.constants import (\n",
    "    EEE_COUNTRIES_FILEPATH,\n",
    "    REPLICATION_PACKAGE_DIR,\n",
    "    TRAFFIC_LOGS_IP_CLASSIFIED_FILEPATH,\n",
    "    ANYCAST_PII_TRAFFIC_LOGS_FILEPATH,\n",
    "    APKS_METADATA_FILEPATH,\n",
    "    RESULTS_MODES,\n",
    "    IT_ANNOTATION_FILEPATH,\n",
    "    TPLS_RESULTS_FILEPATH\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-13T22:32:04.376021Z",
     "start_time": "2024-03-13T22:32:04.372648Z"
    }
   },
   "id": "69d16f8da4f9071f"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Constants\n",
    "EEE_countries_set = set([country[\"alpha-2\"] for country in json_file_to_dict(EEE_COUNTRIES_FILEPATH)])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-13T22:32:04.379357Z",
     "start_time": "2024-03-13T22:32:04.376917Z"
    }
   },
   "id": "adb458e6157c0f69"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Analysis params\n",
    "DESTINATION_REPETITIONS_LIMIT = 1\n",
    "ANALYSIS_MODE=RESULTS_MODES[0]\n",
    "GENERATE_ROUTES_DATA=False"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-13T22:32:04.382760Z",
     "start_time": "2024-03-13T22:32:04.380712Z"
    }
   },
   "id": "690c1391498feefb"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Filepaths variables\n",
    "EXPERIMENT_RESULTS_FOLDER = f\"{REPLICATION_PACKAGE_DIR}/experiment_results_{ANALYSIS_MODE}\"\n",
    "ANALYSIS_FOLDER = f\"{REPLICATION_PACKAGE_DIR}/analysis_{ANALYSIS_MODE}\"\n",
    "\n",
    "ROUTES_RESULTS_FILENAME = f\"{ANALYSIS_FOLDER}/routes_results_{ANALYSIS_MODE}.csv\"\n",
    "ROUTES_FREQUENCY_FILENAME = f\"{ANALYSIS_FOLDER}/routes_frequency_{ANALYSIS_MODE}.csv\"\n",
    "\n",
    "ANYCAST_PII_TRAFFIC_LOGS_ANALYSIS_FILEPATH = f\"{REPLICATION_PACKAGE_DIR}/analysis_{ANALYSIS_MODE}/Anycast_PII_Traffic_Logs_{ANALYSIS_MODE}.csv\"\n",
    "TRAFFIC_LOGS_IP_CLASSIFIED_ANALYSIS_FILEPATH = f\"{REPLICATION_PACKAGE_DIR}/analysis_{ANALYSIS_MODE}/Traffic_logs_10K_ip_classified_{ANALYSIS_MODE}.csv\"\n",
    "ANYCAST_PII_TRAFFIC_LOGS_ANALYSIS_AGGREGATION_FILEPATH = f\"{REPLICATION_PACKAGE_DIR}/analysis_{ANALYSIS_MODE}/Anycast_PII_Traffic_Logs_aggregation_{ANALYSIS_MODE}.csv\"\n",
    "TRAFFIC_LOGS_IP_CLASSIFIED_ANALYSIS_AGGREGATION_FILEPATH = f\"{REPLICATION_PACKAGE_DIR}/analysis_{ANALYSIS_MODE}/Traffic_logs_10K_ip_classified_aggregation_{ANALYSIS_MODE}.csv\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-13T22:32:04.388768Z",
     "start_time": "2024-03-13T22:32:04.383613Z"
    }
   },
   "id": "759dc69c48ceb4bd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Enrichment of data and generation of datasets"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "50b5569c9c01bcd1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Generate a file with all the routes got as the result of the experiment execution"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "829bbc8293d73e11"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Auxiliary functions\n",
    "def get_probe_location(probe_id: int, origin_list: []) -> (float, float):\n",
    "    for origin in origin_list:\n",
    "        if probe_id == origin[\"probe_id\"]:\n",
    "            location = from_geojson(origin[\"location\"])\n",
    "            return location.y, location.x\n",
    "        else:\n",
    "            continue\n",
    "    return 0, 0\n",
    "\n",
    "def get_result_country_route(hunter_result: dict) -> dict:\n",
    "    probe_id = hunter_result[\"origin_id\"]\n",
    "    result_country = hunter_result[\"location_result\"][\"country\"]\n",
    "    probe_country = hunter_result[\"origin_country_code\"]\n",
    "\n",
    "    return {\n",
    "        \"origin_id\": probe_id,\n",
    "        \"origin_country\": probe_country,\n",
    "        \"result_country\": result_country\n",
    "    }\n",
    "\n",
    "# Data generation function\n",
    "def generate_routes_raw():\n",
    "    routes_raw_df = pd.DataFrame(\n",
    "        columns=[\n",
    "            \"target\", \"probe_id\", \"ips_previous_to_target\",\n",
    "            \"origin_country\", \"origin_latitude\", \"origin_longitude\", \n",
    "            \"result_country\", \"result_latitude\", \"result_longitude\",\n",
    "            \"result_filename\", \"outside_EEE\"\n",
    "        ]\n",
    "    )\n",
    "    for result_filename in get_list_files_in_path(EXPERIMENT_RESULTS_FOLDER):\n",
    "        print(result_filename)\n",
    "        result = json_file_to_dict(f\"{EXPERIMENT_RESULTS_FOLDER}/{result_filename}\")\n",
    "        target = result[\"target\"]\n",
    "        origin_list = result[\"measurements\"][\"origin\"]\n",
    "        for hunter_result in result[\"hunter_results\"]:\n",
    "            route = get_result_country_route(hunter_result)\n",
    "            probe_id = hunter_result[\"origin_id\"]\n",
    "            origin_country = route[\"origin_country\"]\n",
    "            origin_latitude, origin_longitude = get_probe_location(probe_id, origin_list)\n",
    "            \n",
    "            result_country = route[\"result_country\"]\n",
    "            if len(hunter_result[\"location_result\"][\"airports_intersection\"]) == 1:\n",
    "                result_location = from_geojson(hunter_result[\"location_result\"][\"airports_intersection\"][0][\"location\"])\n",
    "                result_latitude = result_location.y\n",
    "                result_longitude = result_location.x\n",
    "            else:\n",
    "                result_latitude = 0\n",
    "                result_longitude = 0\n",
    "                \n",
    "            outside_eee = (result_country not in EEE_countries_set) and (result_country != \"Indeterminate\")\n",
    "            \n",
    "            if result_country == \"Indeterminate\":\n",
    "                ips_previous_to_target = [\"Indeterminate\"]\n",
    "            else:\n",
    "                ips_previous_to_target = [\n",
    "                    ip[\"ip\"]\n",
    "                    for ip in hunter_result[\"ips_previous_to_target\"]\n",
    "                ]\n",
    "            \n",
    "            routes_raw_df = pd.concat(\n",
    "                [pd.DataFrame([[\n",
    "                    target, probe_id, str(ips_previous_to_target),\n",
    "                    origin_country, origin_latitude, origin_longitude, \n",
    "                    result_country, result_latitude, result_longitude,\n",
    "                    result_filename, outside_eee\n",
    "                ]], columns=routes_raw_df.columns), routes_raw_df], \n",
    "                ignore_index=True\n",
    "            )\n",
    "    # Sort and save\n",
    "    routes_raw_df.sort_values(by=[\"target\", \"origin_country\", \"result_country\"], inplace=True)\n",
    "    routes_raw_df.to_csv(ROUTES_RESULTS_FILENAME, sep=\",\", index=False)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3a4a5ff135ebc9cc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Aggregate and count routes repetitions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2905481c5a38b1ac"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Data generation function\n",
    "def generate_routes_frequency_aggregation():\n",
    "    # Aggregate routes counting the repetitions\n",
    "    routes_frequency_df = pd.read_csv(ROUTES_RESULTS_FILENAME, sep=\",\")\n",
    "    routes_frequency_df = routes_frequency_df[[\"target\", \"origin_country\", \"result_country\"]]\n",
    "    routes_frequency_df = routes_frequency_df.value_counts(subset=['target', 'origin_country', 'result_country'])\n",
    "    routes_frequency_df.to_csv(ROUTES_FREQUENCY_FILENAME, sep=\",\")\n",
    "\n",
    "    # Include the info about outside the EEE\n",
    "    routes_frequency_df = pd.read_csv(ROUTES_FREQUENCY_FILENAME, sep=\",\")\n",
    "    routes_frequency_df[\"outside_EEE\"] = False\n",
    "    \n",
    "    for index, row in routes_frequency_df.iterrows():\n",
    "        result_country = row[\"result_country\"]\n",
    "        routes_frequency_df.loc[index, \"outside_EEE\"] = (result_country not in EEE_countries_set) and (result_country != \"Indeterminate\")\n",
    "    \n",
    "    routes_frequency_df.sort_values(by=[\"target\", \"origin_country\", \"result_country\"], inplace=True)\n",
    "    routes_frequency_df.to_csv(ROUTES_FREQUENCY_FILENAME, sep=\",\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-13T22:32:04.404975Z",
     "start_time": "2024-03-13T22:32:04.398604Z"
    }
   },
   "id": "1ef13530479b1cbb",
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "Introduce routes outside EEE and its count in the complete dataset WHAT DATASETS POPULATE????"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b51ebb6a53d3489d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Dataset population function\n",
    "def populate_dataset_with_routes_results(dataframe: pd.DataFrame):\n",
    "    routes_valid_df = pd.read_csv(ROUTES_FREQUENCY_FILENAME, sep=\",\")\n",
    "    routes_valid_df = routes_valid_df.loc[\n",
    "        (routes_valid_df[\"outside_EEE\"] == True) & \n",
    "        (routes_valid_df[\"count\"] >= DESTINATION_REPETITIONS_LIMIT)\n",
    "    ]\n",
    "    \n",
    "    routes_valid_dict = {}\n",
    "    for index, row in routes_valid_df.iterrows():\n",
    "        target = row[\"target\"]\n",
    "        if target not in routes_valid_dict.keys():\n",
    "            routes_valid_dict[row[\"target\"]] = {\n",
    "                \"origins\": [],\n",
    "                \"destinations\": [],\n",
    "                \"count\": []\n",
    "            }\n",
    "        \n",
    "        routes_valid_dict[target][\"origins\"].append(row[\"origin_country\"])\n",
    "        routes_valid_dict[target][\"destinations\"].append(row[\"result_country\"])\n",
    "        routes_valid_dict[target][\"count\"].append(str(row[\"count\"]))\n",
    "    \n",
    "    # Charge routes in the complete dataset\n",
    "    dataframe[\"origins_transfers_outside_EEE\"] = \"[]\"\n",
    "    dataframe[\"destinations_transfers_outside_EEE\"] = \"[]\"\n",
    "    dataframe[\"frequency_transfers_outside_EEE\"] = \"[]\"\n",
    "    dataframe[\"outside_EEE\"] = False\n",
    "    \n",
    "    for target in routes_valid_dict.keys():\n",
    "        dataframe.loc[\n",
    "            (dataframe[\"ip_dest\"] == target), [\"origins_transfers_outside_EEE\"]\n",
    "        ] = str(routes_valid_dict[target][\"origins\"])\n",
    "        dataframe.loc[\n",
    "            (dataframe[\"ip_dest\"] == target), [\"destinations_transfers_outside_EEE\"]\n",
    "        ] = str(routes_valid_dict[target][\"destinations\"])\n",
    "        dataframe.loc[\n",
    "            (dataframe[\"ip_dest\"] == target), [\"frequency_transfers_outside_EEE\"]\n",
    "        ] = str(routes_valid_dict[target][\"count\"])\n",
    "        dataframe.loc[\n",
    "            (dataframe[\"ip_dest\"] == target), [\"outside_EEE\"]\n",
    "        ] = True\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-13T22:32:04.413524Z",
     "start_time": "2024-03-13T22:32:04.406402Z"
    }
   },
   "id": "c5fdd5566a17a165",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "Populate the datasets with metadata info"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f4e872e6e4f5f347"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def populate_dataset_with_apks_metadata(dataframe: pd.DataFrame):\n",
    "    apk_metadata_df = pd.read_csv(APKS_METADATA_FILEPATH, sep=\",\")\n",
    "    \n",
    "    for index, row in apk_metadata_df.iterrows():\n",
    "        dataframe.loc[\n",
    "            dataframe[\"apk\"] == row[\"apk\"], \"android_rating\"\n",
    "        ] = row[\"android_rating\"]\n",
    "    \n",
    "        dataframe.loc[\n",
    "            dataframe[\"apk\"] == row[\"apk\"], \"android_numDownloads\"\n",
    "        ] = row[\"android_numDownloads\"]\n",
    "    \n",
    "        dataframe.loc[\n",
    "            dataframe[\"apk\"] == row[\"apk\"], \"android_category\"\n",
    "        ] = row[\"android_category\"]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-13T22:32:04.417894Z",
     "start_time": "2024-03-13T22:32:04.414562Z"
    }
   },
   "id": "6b1c5cf65806854e",
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "Populate the datasets with the info extrated from every policy"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cce1d5fd8c0c621b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def populate_dataset_with_policy_extracted_info(dataframe: pd.DataFrame):\n",
    "    it_annotation_results_df = pd.read_csv(IT_ANNOTATION_FILEPATH, sep=\",\")\n",
    "    \n",
    "    it_annotation_results_df.drop_duplicates([\"apk\", \"countries\"], inplace=True)\n",
    "    for index, row in it_annotation_results_df.iterrows():\n",
    "        if row[\"countries\"]:\n",
    "            countries_mentioned_by_policy = row[\"countries\"]\n",
    "        else:\n",
    "            countries_mentioned_by_policy = \"[]\"\n",
    "        dataframe.loc[\n",
    "            dataframe[\"apk\"] == row[\"apk\"], \"countries_mentioned_by_policy\"\n",
    "        ] = countries_mentioned_by_policy\n",
    "\n",
    "        dataframe.loc[\n",
    "            dataframe[\"apk\"] == row[\"apk\"], \"it_mentioned_by_policy\"\n",
    "        ] = row[\"transfer\"]\n",
    "        \n",
    "        dataframe.loc[\n",
    "            dataframe[\"apk\"] == row[\"apk\"], \"adequacy_decision_by_policy\"\n",
    "        ] = row[\"adequacy_decision\"]\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-13T22:32:04.423038Z",
     "start_time": "2024-03-13T22:32:04.419512Z"
    }
   },
   "id": "4b79f08eb4c84797",
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "Populate the datasets with the info about the libraries which carried out the communication "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "da2d74b9a83b5b19"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def populate_dataset_with_libraries_data(dataframe: pd.DataFrame):\n",
    "    tpls_results_df = pd.read_csv(TPLS_RESULTS_FILEPATH, sep=\",\")\n",
    "    tpls_results_df.drop_duplicates(inplace=True)\n",
    "    \n",
    "    dataframe = pd.merge(\n",
    "        dataset, \n",
    "        tpls_results_df[['apk', 'ip_dest', 'stackTrace', 'TP-performed', 'TP-library', 'FP-intended']], \n",
    "        how=\"left\")\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-13T22:32:04.427177Z",
     "start_time": "2024-03-13T22:32:04.424240Z"
    }
   },
   "id": "f4c048778787baae",
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "source": [
    "Check GDPR compliance in terms of international transfers"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a790c34e9a0bfc98"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def check_it_gdpr_compliance(dataframe: pd.DataFrame):\n",
    "    for index, row in dataframe.iterrows():\n",
    "        destinations_transfers_outside_eee_raw = row[\"destinations_transfers_outside_EEE\"]\n",
    "        if type(destinations_transfers_outside_eee_raw) is str:\n",
    "            try:\n",
    "                destinations_transfers_outside_eee = set(literal_eval(destinations_transfers_outside_eee_raw))\n",
    "            except:\n",
    "                destinations_transfers_outside_eee = set()\n",
    "        else:\n",
    "            destinations_transfers_outside_eee = set()\n",
    "        \n",
    "        countries_mentioned_by_policy_raw = row[\"countries_mentioned_by_policy\"]\n",
    "        if type(countries_mentioned_by_policy_raw) is str:\n",
    "            try:\n",
    "                countries_mentioned_by_policy = set(literal_eval(countries_mentioned_by_policy_raw))\n",
    "            except:\n",
    "                countries_mentioned_by_policy = set()\n",
    "        else:\n",
    "            countries_mentioned_by_policy = set()\n",
    "        \n",
    "        if not destinations_transfers_outside_eee:\n",
    "            dataframe.loc[index, \"apk_it_gdpr_compliance\"] = True\n",
    "        else:\n",
    "            if destinations_transfers_outside_eee.issubset(countries_mentioned_by_policy):\n",
    "                dataframe.loc[index, \"apk_it_gdpr_compliance\"] = True\n",
    "            else:\n",
    "                dataframe.loc[index, \"apk_it_gdpr_compliance\"] = False\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-13T22:32:04.434535Z",
     "start_time": "2024-03-13T22:32:04.429542Z"
    }
   },
   "id": "918ecd5bac3f9b42",
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "source": [
    "Aggregate analysis"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "afd295a3dbb8c704"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def aggregate_analysis(dataframe: pd.DataFrame, to_filepath: str):\n",
    "    aggregation_df = dataframe[\n",
    "        [\"apk\", \"version\", \"phase\", \n",
    "         \"tls\", \"https\", \"host\", \"ip_dest\", \n",
    "         \"PII\", \"ip_anycast\", \n",
    "         \"origins_transfers_outside_EEE\", \"destinations_transfers_outside_EEE\", \"frequency_transfers_outside_EEE\", \"outside_EEE\",\n",
    "         \"android_rating\", \"android_numDownloads\", \"android_category\", \n",
    "         \"countries_mentioned_by_policy\", \"it_mentioned_by_policy\", \"adequacy_decision_by_policy\", \"it_gdpr_compliance\"]\n",
    "    ].copy()\n",
    "    \n",
    "    aggregation_df.drop_duplicates(inplace=True)\n",
    "\n",
    "    aggregation_df.fillna(\n",
    "        value={\n",
    "            \"countries_mentioned_by_policy\": \"[]\",\n",
    "            \"it_mentioned_by_policy\": False,\n",
    "            \"adequacy_decision_by_policy\": False\n",
    "        }, inplace=True\n",
    "    )\n",
    "    \n",
    "    aggregation_df.to_csv(to_filepath, sep=\",\", index=False)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-13T22:32:04.442957Z",
     "start_time": "2024-03-13T22:32:04.438822Z"
    }
   },
   "id": "b0effef8193a7b8e",
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": [
    "Execute the enrichment of data and generation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8f23ffaf42779f8b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Charge the dataframes to be used\n",
    "traffic_logs_ip_classified_analysis_df = pd.read_csv(TRAFFIC_LOGS_IP_CLASSIFIED_FILEPATH, sep=\",\")\n",
    "anycast_pii_traffic_logs_df = pd.read_csv(ANYCAST_PII_TRAFFIC_LOGS_FILEPATH, sep=\",\")\n",
    "datasets = [traffic_logs_ip_classified_analysis_df, anycast_pii_traffic_logs_df]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-13T22:32:06.019314Z",
     "start_time": "2024-03-13T22:32:04.473007Z"
    }
   },
   "id": "ca54aaa1dc002700",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Because is a long process and is only necessary to run once I include the condition\n",
    "if GENERATE_ROUTES_DATA:\n",
    "    # Generate the results\n",
    "    generate_routes_raw()\n",
    "    generate_routes_frequency_aggregation()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-13T22:32:06.029146Z",
     "start_time": "2024-03-13T22:32:06.020795Z"
    }
   },
   "id": "1b453eecfba99303",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populate with routes\n",
      "Populate with metadata\n",
      "Populate with the privacy policy extracted data\n",
      "Populate with the libraries data\n",
      "Populate with routes\n",
      "Populate with metadata\n",
      "Populate with the privacy policy extracted data\n",
      "Populate with the libraries data\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You are trying to merge on float64 and object columns for key 'stackTrace'. If you wish to proceed you should use pd.concat",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[16], line 14\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;66;03m# Populate with libraries data\u001B[39;00m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPopulate with the libraries data\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 14\u001B[0m \u001B[43mpopulate_dataset_with_libraries_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m)\u001B[49m    \n",
      "Cell \u001B[0;32mIn[11], line 5\u001B[0m, in \u001B[0;36mpopulate_dataset_with_libraries_data\u001B[0;34m(dataframe)\u001B[0m\n\u001B[1;32m      2\u001B[0m tpls_results_df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(TPLS_RESULTS_FILEPATH, sep\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m,\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      3\u001B[0m tpls_results_df\u001B[38;5;241m.\u001B[39mdrop_duplicates(inplace\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m----> 5\u001B[0m dataframe \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmerge\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtpls_results_df\u001B[49m\u001B[43m[\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mapk\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mip_dest\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mstackTrace\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mTP-performed\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mTP-library\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mFP-intended\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhow\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mleft\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/dev/hunter_experiment_anycast_europe/venv/lib/python3.10/site-packages/pandas/core/reshape/merge.py:170\u001B[0m, in \u001B[0;36mmerge\u001B[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001B[0m\n\u001B[1;32m    155\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _cross_merge(\n\u001B[1;32m    156\u001B[0m         left_df,\n\u001B[1;32m    157\u001B[0m         right_df,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    167\u001B[0m         copy\u001B[38;5;241m=\u001B[39mcopy,\n\u001B[1;32m    168\u001B[0m     )\n\u001B[1;32m    169\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 170\u001B[0m     op \u001B[38;5;241m=\u001B[39m \u001B[43m_MergeOperation\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    171\u001B[0m \u001B[43m        \u001B[49m\u001B[43mleft_df\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    172\u001B[0m \u001B[43m        \u001B[49m\u001B[43mright_df\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    173\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhow\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhow\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    174\u001B[0m \u001B[43m        \u001B[49m\u001B[43mon\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mon\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    175\u001B[0m \u001B[43m        \u001B[49m\u001B[43mleft_on\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mleft_on\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    176\u001B[0m \u001B[43m        \u001B[49m\u001B[43mright_on\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mright_on\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    177\u001B[0m \u001B[43m        \u001B[49m\u001B[43mleft_index\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mleft_index\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    178\u001B[0m \u001B[43m        \u001B[49m\u001B[43mright_index\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mright_index\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    179\u001B[0m \u001B[43m        \u001B[49m\u001B[43msort\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msort\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    180\u001B[0m \u001B[43m        \u001B[49m\u001B[43msuffixes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msuffixes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    181\u001B[0m \u001B[43m        \u001B[49m\u001B[43mindicator\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mindicator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    182\u001B[0m \u001B[43m        \u001B[49m\u001B[43mvalidate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvalidate\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    183\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    184\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m op\u001B[38;5;241m.\u001B[39mget_result(copy\u001B[38;5;241m=\u001B[39mcopy)\n",
      "File \u001B[0;32m~/dev/hunter_experiment_anycast_europe/venv/lib/python3.10/site-packages/pandas/core/reshape/merge.py:807\u001B[0m, in \u001B[0;36m_MergeOperation.__init__\u001B[0;34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001B[0m\n\u001B[1;32m    803\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_tolerance(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mleft_join_keys)\n\u001B[1;32m    805\u001B[0m \u001B[38;5;66;03m# validate the merge keys dtypes. We may need to coerce\u001B[39;00m\n\u001B[1;32m    806\u001B[0m \u001B[38;5;66;03m# to avoid incompatible dtypes\u001B[39;00m\n\u001B[0;32m--> 807\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_maybe_coerce_merge_keys\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    809\u001B[0m \u001B[38;5;66;03m# If argument passed to validate,\u001B[39;00m\n\u001B[1;32m    810\u001B[0m \u001B[38;5;66;03m# check if columns specified as unique\u001B[39;00m\n\u001B[1;32m    811\u001B[0m \u001B[38;5;66;03m# are in fact unique.\u001B[39;00m\n\u001B[1;32m    812\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m validate \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/dev/hunter_experiment_anycast_europe/venv/lib/python3.10/site-packages/pandas/core/reshape/merge.py:1508\u001B[0m, in \u001B[0;36m_MergeOperation._maybe_coerce_merge_keys\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1502\u001B[0m     \u001B[38;5;66;03m# unless we are merging non-string-like with string-like\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m (\n\u001B[1;32m   1504\u001B[0m         inferred_left \u001B[38;5;129;01min\u001B[39;00m string_types \u001B[38;5;129;01mand\u001B[39;00m inferred_right \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m string_types\n\u001B[1;32m   1505\u001B[0m     ) \u001B[38;5;129;01mor\u001B[39;00m (\n\u001B[1;32m   1506\u001B[0m         inferred_right \u001B[38;5;129;01min\u001B[39;00m string_types \u001B[38;5;129;01mand\u001B[39;00m inferred_left \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m string_types\n\u001B[1;32m   1507\u001B[0m     ):\n\u001B[0;32m-> 1508\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(msg)\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;66;03m# datetimelikes must match exactly\u001B[39;00m\n\u001B[1;32m   1511\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m needs_i8_conversion(lk\u001B[38;5;241m.\u001B[39mdtype) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m needs_i8_conversion(rk\u001B[38;5;241m.\u001B[39mdtype):\n",
      "\u001B[0;31mValueError\u001B[0m: You are trying to merge on float64 and object columns for key 'stackTrace'. If you wish to proceed you should use pd.concat"
     ]
    }
   ],
   "source": [
    "# Populate datasets\n",
    "for dataset in datasets:\n",
    "    # Populate with routes\n",
    "    print(\"Populate with routes\")\n",
    "    populate_dataset_with_routes_results(dataset)\n",
    "    # Populate with metadata\n",
    "    print(\"Populate with metadata\")\n",
    "    populate_dataset_with_apks_metadata(dataset)\n",
    "    # Populate with the privacy policy extracted data\n",
    "    print(\"Populate with the privacy policy extracted data\")\n",
    "    populate_dataset_with_policy_extracted_info(dataset)\n",
    "    # Populate with libraries data\n",
    "    print(\"Populate with the libraries data\")\n",
    "    populate_dataset_with_libraries_data(dataset)    \n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-13T22:33:34.722053Z",
     "start_time": "2024-03-13T22:32:06.030368Z"
    }
   },
   "id": "b8cc3a8f6866331a",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Check conditions\n",
    "for dataset in datasets:\n",
    "    check_it_gdpr_compliance(dataset)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fb6cbbb47e861175",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Generate datasets aggregated\n",
    "# aggregate_analysis_filepaths = [TRAFFIC_LOGS_IP_CLASSIFIED_ANALYSIS_AGGREGATION_FILEPATH, ANYCAST_PII_TRAFFIC_LOGS_ANALYSIS_AGGREGATION_FILEPATH]\n",
    "# for index in range(0, len(datasets)):\n",
    "#     aggregate_analysis(datasets[index], aggregate_analysis_filepaths[index])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "776ea25af65be427",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    dataset.fillna(\n",
    "        value={\n",
    "            \"countries_mentioned_by_policy\": \"[]\",\n",
    "            \"it_mentioned_by_policy\": False,\n",
    "            \"adequacy_decision_by_policy\": False\n",
    "        }, inplace=True\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "85d172d8a6dc6325",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Save the datasets populated\n",
    "analysis_filepaths = [TRAFFIC_LOGS_IP_CLASSIFIED_ANALYSIS_FILEPATH, ANYCAST_PII_TRAFFIC_LOGS_ANALYSIS_FILEPATH]\n",
    "for index in range(0, len(datasets)):\n",
    "    datasets[index].to_csv(analysis_filepaths[index], sep=\",\", index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fef4c8ad292c6e39",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Analysis questions\n",
    "\n",
    "Answers to the questions needed for the article\n",
    "\n",
    "Acronyms:\n",
    "- PII = Personal Identificable Information"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1bd3273fca7aeeaa"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Data load\n",
    "traffic_logs_ip_classified_analysis_df = pd.read_csv(TRAFFIC_LOGS_IP_CLASSIFIED_ANALYSIS_FILEPATH, sep=\",\")\n",
    "anycast_pii_traffic_logs_analysis_df = pd.read_csv(ANYCAST_PII_TRAFFIC_LOGS_ANALYSIS_FILEPATH, sep=\",\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4312ee087eaabf53",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "**IPs analysis**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "88555225610e9127"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "ips_total = traffic_logs_ip_classified_analysis_df[\"ip_dest\"].unique().tolist()\n",
    "print(f\"Number of IPs in traffic logs: {len(ips_total)}\")\n",
    "\n",
    "ips_total_pii = traffic_logs_ip_classified_analysis_df.loc[\n",
    "    (traffic_logs_ip_classified_analysis_df[\"PII\"] != \"No-PII\") &\n",
    "    (traffic_logs_ip_classified_analysis_df[\"PII\"].notna())\n",
    "    ][\"ip_dest\"].unique().tolist()\n",
    "print(f\"Number of IPs with PII: {len(ips_total_pii)}\")\n",
    "\n",
    "ips_anycast = traffic_logs_ip_classified_analysis_df.loc[\n",
    "    traffic_logs_ip_classified_analysis_df[\"ip_anycast\"]\n",
    "][\"ip_dest\"].unique().tolist()\n",
    "print(f\"Number of IPs anycast: {len(ips_anycast)}\")\n",
    "\n",
    "ips_anycast_pii = traffic_logs_ip_classified_analysis_df.loc[\n",
    "    (traffic_logs_ip_classified_analysis_df[\"ip_anycast\"]) &\n",
    "    (traffic_logs_ip_classified_analysis_df[\"PII\"] != \"No-PII\") &\n",
    "    (traffic_logs_ip_classified_analysis_df[\"PII\"].notna())\n",
    "    ][\"ip_dest\"].unique().tolist()\n",
    "print(f\"Number of IPs anycast with PII: {len(ips_anycast_pii)}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a95eaa1142c4f4b8",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "**APKS analysis**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a71b37155d0389f5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "apks_total = traffic_logs_ip_classified_analysis_df[\"apk\"].unique().tolist()\n",
    "print(f\"Number of APKs in traffic logs: {len(apks_total)}\")\n",
    "\n",
    "apks_total_pii = traffic_logs_ip_classified_analysis_df.loc[\n",
    "    (traffic_logs_ip_classified_analysis_df[\"PII\"] != \"No-PII\") &\n",
    "    (traffic_logs_ip_classified_analysis_df[\"PII\"].notna())\n",
    "    ][\"apk\"].unique().tolist()\n",
    "print(f\"Number of APKs with PII: {len(apks_total_pii)}\")\n",
    "\n",
    "apks_anycast = traffic_logs_ip_classified_analysis_df.loc[\n",
    "    traffic_logs_ip_classified_analysis_df[\"ip_anycast\"]\n",
    "][\"apk\"].unique().tolist()\n",
    "print(f\"Number of APKs using anycast: {len(apks_anycast)}\")\n",
    "\n",
    "apks_anycast_pii = traffic_logs_ip_classified_analysis_df.loc[\n",
    "    (traffic_logs_ip_classified_analysis_df[\"ip_anycast\"]) &\n",
    "    (traffic_logs_ip_classified_analysis_df[\"PII\"] != \"No-PII\") &\n",
    "    (traffic_logs_ip_classified_analysis_df[\"PII\"].notna())\n",
    "    ][\"apk\"].unique().tolist()\n",
    "print(f\"Number of APKs anycast with PII: {len(apks_anycast_pii)}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "76c72f39cb6c7d85",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Hosts Analysis**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c98ce9aa4f43b5d2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "hosts_total = traffic_logs_ip_classified_analysis_df[\"host\"].unique().tolist()\n",
    "print(f\"Number of hosts in traffic logs: {len(hosts_total)}\")\n",
    "\n",
    "hosts_total_pii = traffic_logs_ip_classified_analysis_df.loc[\n",
    "    (traffic_logs_ip_classified_analysis_df[\"PII\"] != \"No-PII\") &\n",
    "    (traffic_logs_ip_classified_analysis_df[\"PII\"].notna())\n",
    "    ][\"host\"].unique().tolist()\n",
    "print(f\"Number of hosts with PII: {len(hosts_total_pii)}\")\n",
    "\n",
    "hosts_anycast = traffic_logs_ip_classified_analysis_df.loc[\n",
    "    traffic_logs_ip_classified_analysis_df[\"ip_anycast\"]\n",
    "][\"host\"].unique().tolist()\n",
    "print(f\"Number of hosts using anycast: {len(hosts_anycast)}\")\n",
    "\n",
    "hosts_anycast_pii = traffic_logs_ip_classified_analysis_df.loc[\n",
    "    (traffic_logs_ip_classified_analysis_df[\"ip_anycast\"]) &\n",
    "    (traffic_logs_ip_classified_analysis_df[\"PII\"] != \"No-PII\") &\n",
    "    (traffic_logs_ip_classified_analysis_df[\"PII\"].notna())\n",
    "    ][\"host\"].unique().tolist()\n",
    "print(f\"Number of hosts anycast with PII: {len(hosts_anycast_pii)}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5a9bcccc70a378d",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Data Types**  "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "22eacec4072b9aef"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "pii_data_types_anycast = anycast_pii_traffic_logs_analysis_df[\"PII\"].unique().tolist()\n",
    "print(f\"Types of PII data treated by anycast IPs:\")\n",
    "print(pii_data_types_anycast)\n",
    "\n",
    "pii_data_types_anycast_pii_it = anycast_pii_traffic_logs_analysis_df.loc[\n",
    "    anycast_pii_traffic_logs_analysis_df[\"outside_EEE\"]\n",
    "][\"PII\"].unique().tolist()\n",
    "print(f\"Types of PII data treated by anycast IPs that make IT:\")\n",
    "print(pii_data_types_anycast_pii_it)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c27f5d9f8c7fe13b",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "**GDPR Compliance**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9d554e1ce805ee86"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "apks_anycast_pii_declare_it = anycast_pii_traffic_logs_analysis_df.loc[\n",
    "    anycast_pii_traffic_logs_analysis_df[\"it_mentioned_by_policy\"]\n",
    "][\"apk\"].unique().tolist()\n",
    "print(f\"Number of APKs that use anycast IPs and treat PII that declare IT in privacy policy: {len(apks_anycast_pii_declare_it)}\")\n",
    "\n",
    "apks_anycast_pii_not_compliance = anycast_pii_traffic_logs_analysis_df.loc[\n",
    "    anycast_pii_traffic_logs_analysis_df[\"apks_it_gdpr_compliance\"] == False\n",
    "][\"apk\"].unique().tolist()\n",
    "print(f\"Number of APKs that use anycast IPs and treat PII that has not compliance: {len(apks_anycast_pii_not_compliance)}\")\n",
    "\n",
    "apks_anycast_pii_not_compliance_declare_it = anycast_pii_traffic_logs_analysis_df.loc[\n",
    "    (anycast_pii_traffic_logs_analysis_df[\"it_mentioned_by_policy\"] == True) &\n",
    "    (anycast_pii_traffic_logs_analysis_df[\"it_gdpr_compliance\"] == False)\n",
    "    ][\"apk\"].unique().tolist()\n",
    "print(f\"Number of APKs that use anycast IPs and treat PII that has not compliance and declare IT in privacy policy: {len(apks_anycast_pii_not_compliance_declare_it)}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "39f65ca46cf5f6ec",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "**TLPs**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "772e34f4c4130bdc"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(\"Libraries\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6c757064eb2128f3",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# STOP"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9361948247880a97"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Crear datos aleatorios para el DataFrame\n",
    "data = {\n",
    "    'Columna1': np.random.randint(0, 100, 10),\n",
    "    'Columna2': np.random.randn(10),\n",
    "    'Columna3': np.random.choice(['A', 'B', 'C'], 10),\n",
    "    'Columna4': np.random.rand(10),\n",
    "    'Columna5': np.random.choice([True, False], 10)\n",
    "}\n",
    "\n",
    "# Crear el DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Mostrar el DataFrame\n",
    "print(df)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e24ae718f8821b47",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df.loc[\n",
    "    (df[\"Columna1\"] == 12) &\n",
    "    (df[\"Columna3\"] == \"B\"),\n",
    "    [\"Columna2\", \"Columna4\"]\n",
    "] = [0]\n",
    "\n",
    "df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ce3fdc8a8aa99b3c",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
